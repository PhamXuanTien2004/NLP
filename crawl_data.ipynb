{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e72341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thu thập 50 link bài viết.\n",
      "Đã lưu 48 bài viết vào file raw_articles.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_vnexpress_articles(num_articles=50):\n",
    "    url = \"https://vnexpress.net\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    links = []\n",
    "\n",
    "    # Lấy các thẻ chứa đường link bài viết chính\n",
    "    for a in soup.select(\"a[href^='https://vnexpress.net']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and href not in links and len(links) < num_articles:\n",
    "            links.append(href)\n",
    "\n",
    "    print(f\"Đã thu thập {len(links)} link bài viết.\")\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            r = requests.get(link, timeout=5)\n",
    "            s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Lấy nội dung chính (thẻ article)\n",
    "            body = s.find(\"article\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "                if len(content) > 100:\n",
    "                    articles.append(content)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Gộp toàn bộ nội dung và lưu\n",
    "    full_text = \"\\n\".join(articles)\n",
    "    with open(\"raw_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "    print(f\"Đã lưu {len(articles)} bài viết vào file raw_articles.txt\")\n",
    "\n",
    "# Gọi hàm crawl\n",
    "crawl_vnexpress_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec78a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số từ sau khi tách: 38979\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_tokenize(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1. Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Loại bỏ số và ký tự đặc biệt (giữ lại chữ cái và khoảng trắng)\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "\n",
    "    # 3. Loại bỏ nhiều khoảng trắng dư thừa\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4. Tách từ\n",
    "    tokens = text.split()\n",
    "\n",
    "    print(f\"Tổng số từ sau khi tách: {len(tokens)}\")\n",
    "    return tokens\n",
    "\n",
    "# Gọi thử\n",
    "tokens = clean_and_tokenize('raw_articles.txt')\n",
    "\n",
    "# (Tùy chọn) Lưu kết quả ra file để xem\n",
    "with open('tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in tokens:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47eb1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tách được 38979 từ.\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Giữ lại chữ cái tiếng Việt và khoảng trắng\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "    \n",
    "    # Loại bỏ khoảng trắng dư\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_with_underthesea(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    cleaned = clean_text(raw_text)\n",
    "\n",
    "    # Tách từ bằng underthesea (giữ đúng từ ghép)\n",
    "    tokenized_text = word_tokenize(cleaned, format=\"text\")  # ra dạng: \"trí_tuệ nhân_tạo là ...\"\n",
    "    \n",
    "    # Chuyển về danh sách từ (loại bỏ dấu _ nếu muốn giữ lại dạng \"trí tuệ\")\n",
    "    tokens = tokenized_text.replace(\"_\", \" \").split()\n",
    "    \n",
    "    print(f\"Đã tách được {len(tokens)} từ.\")\n",
    "    return tokens\n",
    "\n",
    "# Gọi hàm\n",
    "tokens = tokenize_with_underthesea(\"raw_articles.txt\")\n",
    "\n",
    "# Lưu ra file nếu cần\n",
    "with open(\"tokens_underthesea.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in tokens:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dad1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_all_vietnamese_words(folder_path):\n",
    "    all_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        all_words.add(word)\n",
    "    return all_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61f9e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vietnamese_words(tokens, vietnamese_dict):\n",
    "    return [word for word in tokens if word in vietnamese_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "402992a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc 37194 từ tiếng Việt từ tổng 38979 từ.\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn đến thư mục từ điển\n",
    "folder_path = 'tu_dien_goc'\n",
    "\n",
    "# 1. Tải từ điển tiếng Việt từ tất cả file\n",
    "vietnamese_dict = load_all_vietnamese_words(folder_path)\n",
    "\n",
    "# 2. Giả sử bạn có tokens từ file tokens_underthesea.txt\n",
    "with open('tokens_underthesea.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# 3. Lọc từ\n",
    "vietnamese_only_tokens = filter_vietnamese_words(tokens, vietnamese_dict)\n",
    "\n",
    "# 4. Loại bỏ trùng lặp bằng set()\n",
    "unique_tokens = sorted(set(vietnamese_only_tokens))\n",
    "\n",
    "# 5. Ghi ra file\n",
    "with open('vietnamese_only_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in vietnamese_only_tokens:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Đã lọc {len(vietnamese_only_tokens)} từ tiếng Việt từ tổng {len(tokens)} từ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe4a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
