{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e72341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thu thập 50 link bài viết.\n",
      "Đã lưu 50 bài viết vào file raw_articles.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_vnexpress_articles(num_articles=50):\n",
    "    url = \"https://vnexpress.net\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    links = []\n",
    "\n",
    "    # Lấy các thẻ chứa đường link bài viết chính\n",
    "    for a in soup.select(\"a[href^='https://vnexpress.net']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and href not in links and len(links) < num_articles:\n",
    "            links.append(href)\n",
    "\n",
    "    print(f\"Đã thu thập {len(links)} link bài viết.\")\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            r = requests.get(link, timeout=5)\n",
    "            s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Lấy nội dung chính (thẻ article)\n",
    "            body = s.find(\"article\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "                if len(content) > 100:\n",
    "                    articles.append(content)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Gộp toàn bộ nội dung và lưu\n",
    "    full_text = \"\\n\".join(articles)\n",
    "    with open(\"raw_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "    print(f\"Đã lưu {len(articles)} bài viết vào file raw_articles.txt\")\n",
    "\n",
    "# Gọi hàm crawl\n",
    "crawl_vnexpress_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74432d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc và sắp xếp 1646 từ tiếng Việt duy nhất.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from underthesea import word_tokenize\n",
    "import os\n",
    "\n",
    "# 1. Đưa về chữ thường, loại bỏ dấu câu, số, ký tự đặc biệt, loại bỏ khoảng trắng dư thừa\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 2. Đọc dữ liệu thô và làm sạch\n",
    "with open('raw_articles.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# 3. Tách từ bằng underthesea\n",
    "tokenized_text = word_tokenize(cleaned_text, format=\"text\")\n",
    "tokens = tokenized_text.replace(\"_\", \" \").split()\n",
    "\n",
    "# 4. Lọc từ tiếng Việt\n",
    "def load_all_vietnamese_words(folder_path):\n",
    "    all_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        all_words.add(word)\n",
    "    return all_words\n",
    "\n",
    "folder_path = 'tu_dien_goc'  # Thư mục chứa các file từ điển\n",
    "vietnamese_dict = load_all_vietnamese_words(folder_path)\n",
    "vietnamese_only_tokens = [word for word in tokens if word in vietnamese_dict]\n",
    "\n",
    "# 5. Loại bỏ từ trùng lặp và sắp xếp theo thứ tự abc\n",
    "unique_sorted = sorted(set(vietnamese_only_tokens), key=lambda x: x.lower())\n",
    "\n",
    "# 6. Lưu kết quả ra file\n",
    "with open('vietnamese_only_tokens_sorted.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in unique_sorted:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Đã lọc và sắp xếp {len(unique_sorted)} từ tiếng Việt duy nhất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a376d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import unicodedata\n",
    "\n",
    "vowels = \"aăâeêioôơuưy\"\n",
    "tone_marks = [\"́\", \"̀\", \"̉\", \"̃\", \"̣\"]\n",
    "\n",
    "def remove_tone(char):\n",
    "    decomposed = unicodedata.normalize('NFD', char)\n",
    "    base = ''.join([c for c in decomposed if c not in tone_marks])\n",
    "    return unicodedata.normalize('NFC', base)\n",
    "\n",
    "def random_tone(char):\n",
    "    if char not in vowels:\n",
    "        return char\n",
    "    base = unicodedata.normalize('NFD', char)[0]\n",
    "    tone = random.choice(tone_marks)\n",
    "    return unicodedata.normalize('NFC', base + tone)\n",
    "\n",
    "def confused_initials(word):\n",
    "    # Các nhóm phụ âm đầu dễ nhầm lẫn\n",
    "    confusion_groups = [\n",
    "        [\"ch\", \"tr\"],\n",
    "        [\"l\", \"n\"],\n",
    "        [\"d\", \"r\", \"gi\"],\n",
    "        [\"s\", \"x\"],\n",
    "        [\"c\", \"k\"],\n",
    "        [\"ngh\", \"ng\",\"gh\"],\n",
    "    ]\n",
    "    results = []\n",
    "    for group in confusion_groups:\n",
    "        for prefix in group:\n",
    "            if word.startswith(prefix):\n",
    "                for alt in group:\n",
    "                    if alt != prefix:\n",
    "                        results.append(alt + word[len(prefix):])\n",
    "                return results  # chỉ đổi 1 nhóm đầu tiên tìm thấy\n",
    "    return []\n",
    "\n",
    "def generate_typos(word, num_typos=10):\n",
    "    typo_list = []\n",
    "    operations = [\n",
    "        \"repeat\",      # Thừa chữ\n",
    "        \"delete\",      # Thiếu chữ\n",
    "        \"remove_tone\", # Thiếu dấu\n",
    "        \"wrong_tone\",  # Sai dấu\n",
    "        \"transpose\",   # Sai thứ tự chữ\n",
    "        \"confused_initial\", # Nhầm phụ âm đầu\n",
    "    ]\n",
    "    for _ in range(num_typos):\n",
    "        op = random.choice(operations)\n",
    "        typo = None\n",
    "        if op == \"repeat\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos] + word[pos:]\n",
    "        elif op == \"delete\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos+1:]\n",
    "        elif op == \"remove_tone\" and any(c in vowels for c in word):\n",
    "            pos = random.choice([i for i, c in enumerate(word) if c in vowels])\n",
    "            typo = word[:pos] + remove_tone(word[pos]) + word[pos+1:]\n",
    "        elif op == \"wrong_tone\" and any(c in vowels for c in word):\n",
    "            pos = random.choice([i for i, c in enumerate(word) if c in vowels])\n",
    "            typo = word[:pos] + random_tone(word[pos]) + word[pos+1:]\n",
    "        elif op == \"transpose\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 2)\n",
    "            typo = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        elif op == \"confused_initial\":\n",
    "            confused = confused_initials(word)\n",
    "            if confused:\n",
    "                typo = random.choice(confused)\n",
    "        if typo and typo != word and typo not in typo_list:\n",
    "            typo_list.append(typo)\n",
    "    return typo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc danh sách từ\n",
    "with open(\"vietnamese_only_tokens_sorted.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Sinh lỗi cho từng từ và lưu ra file\n",
    "with open(\"typos_for_all_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in words:\n",
    "        typos = generate_typos(word, num_typos=50)  \n",
    "        for typo in typos:\n",
    "            f.write(f\"{word}\\t{typo}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (venv39)",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
