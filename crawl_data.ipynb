{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e72341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thu thập 50 link bài viết.\n",
      "Đã lưu 48 bài viết vào file raw_articles.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_vnexpress_articles(num_articles=50):\n",
    "    url = \"https://vnexpress.net\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    links = []\n",
    "\n",
    "    # Lấy các thẻ chứa đường link bài viết chính\n",
    "    for a in soup.select(\"a[href^='https://vnexpress.net']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and href not in links and len(links) < num_articles:\n",
    "            links.append(href)\n",
    "\n",
    "    print(f\"Đã thu thập {len(links)} link bài viết.\")\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            r = requests.get(link, timeout=5)\n",
    "            s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Lấy nội dung chính (thẻ article)\n",
    "            body = s.find(\"article\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "                if len(content) > 100:\n",
    "                    articles.append(content)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Gộp toàn bộ nội dung và lưu\n",
    "    full_text = \"\\n\".join(articles)\n",
    "    with open(\"raw_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "    print(f\"Đã lưu {len(articles)} bài viết vào file raw_articles.txt\")\n",
    "\n",
    "# Gọi hàm crawl\n",
    "crawl_vnexpress_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec78a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số từ sau khi tách: 30277\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_tokenize(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1. Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Loại bỏ số và ký tự đặc biệt (giữ lại chữ cái và khoảng trắng)\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "\n",
    "    # 3. Loại bỏ nhiều khoảng trắng dư thừa\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 4. Tách từ\n",
    "    tokens = text.split()\n",
    "\n",
    "    print(f\"Tổng số từ sau khi tách: {len(tokens)}\")\n",
    "    return tokens\n",
    "\n",
    "# Gọi thử\n",
    "tokens = clean_and_tokenize('raw_articles.txt')\n",
    "\n",
    "# (Tùy chọn) Lưu kết quả ra file để xem\n",
    "with open('tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in tokens:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47eb1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tách được 30277 từ.\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    # Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Giữ lại chữ cái tiếng Việt và khoảng trắng\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "    \n",
    "    # Loại bỏ khoảng trắng dư\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_with_underthesea(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    cleaned = clean_text(raw_text)\n",
    "\n",
    "    # Tách từ bằng underthesea (giữ đúng từ ghép)\n",
    "    tokenized_text = word_tokenize(cleaned, format=\"text\")  # ra dạng: \"trí_tuệ nhân_tạo là ...\"\n",
    "    \n",
    "    # Chuyển về danh sách từ (loại bỏ dấu _ nếu muốn giữ lại dạng \"trí tuệ\")\n",
    "    tokens = tokenized_text.replace(\"_\", \" \").split()\n",
    "    \n",
    "    print(f\"Đã tách được {len(tokens)} từ.\")\n",
    "    return tokens\n",
    "\n",
    "# Gọi hàm\n",
    "tokens = tokenize_with_underthesea(\"raw_articles.txt\")\n",
    "\n",
    "# Lưu ra file nếu cần\n",
    "with open(\"tokens_underthesea.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in tokens:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dad1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_all_vietnamese_words(folder_path):\n",
    "    all_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        all_words.add(word)\n",
    "    return all_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f9e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vietnamese_words(tokens, vietnamese_dict):\n",
    "    return [word for word in tokens if word in vietnamese_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "402992a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc 28899 từ tiếng Việt từ tổng 30277 từ.\n"
     ]
    }
   ],
   "source": [
    "# Đường dẫn đến thư mục từ điển\n",
    "folder_path = 'tu_dien_goc'\n",
    "\n",
    "# 1. Tải từ điển tiếng Việt từ tất cả file\n",
    "vietnamese_dict = load_all_vietnamese_words(folder_path)\n",
    "\n",
    "# 2. Giả sử bạn có tokens từ file tokens_underthesea.txt\n",
    "with open('tokens_underthesea.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# 3. Lọc từ\n",
    "vietnamese_only_tokens = filter_vietnamese_words(tokens, vietnamese_dict)\n",
    "\n",
    "# 4. Loại bỏ trùng lặp bằng set()\n",
    "unique_tokens = sorted(set(vietnamese_only_tokens))\n",
    "\n",
    "# 5. Ghi ra file\n",
    "with open('vietnamese_only_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in vietnamese_only_tokens:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Đã lọc {len(vietnamese_only_tokens)} từ tiếng Việt từ tổng {len(tokens)} từ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fbe4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc file, loại bỏ dòng trống, sắp xếp và ghi lại file\n",
    "\n",
    "with open(\"vietnamese_only_tokens.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "words_sorted = sorted(words, key=lambda x: x.lower())\n",
    "unique_sorted = sorted(set(words_sorted), key=lambda x: x.lower())\n",
    "\n",
    "\n",
    "with open(\"vietnamese_only_tokens_sorted.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in unique_sorted:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a376d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import unicodedata\n",
    "\n",
    "# Bảng các nguyên âm tiếng Việt và dấu thanh\n",
    "vowels = \"aăâeêioôơuưy\"\n",
    "tone_marks = [\"`\", \"́\", \"̀\", \"̉\", \"̃`\", \"̣\"]  # sắc, huyền, hỏi, ngã, nặng\n",
    "\n",
    "def random_tone(char):\n",
    "    # Thêm hoặc thay đổi dấu thanh cho nguyên âm\n",
    "    if char not in vowels:\n",
    "        return char\n",
    "    base = unicodedata.normalize('NFD', char)[0]\n",
    "    tone = random.choice(tone_marks)\n",
    "    return unicodedata.normalize('NFC', base + tone)\n",
    "\n",
    "def generate_typos(word, num_typos=10):\n",
    "    typo_list = []\n",
    "    operations = [\n",
    "        \"insert\",      # Thêm ký tự ngẫu nhiên\n",
    "        \"delete\",      # Thiếu ký tự\n",
    "        \"substitute\",  # Thay ký tự ngẫu nhiên\n",
    "        \"transpose\",   # Đổi chỗ ký tự\n",
    "        \"repeat\",      # Thừa ký tự\n",
    "        \"wrong_tone\",  # Sai dấu\n",
    "    ]\n",
    "    for _ in range(num_typos):\n",
    "        op = random.choice(operations)\n",
    "        typo = None\n",
    "        if op == \"insert\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word))\n",
    "            char = chr(random.randint(97, 122))\n",
    "            typo = word[:pos] + char + word[pos:]\n",
    "        elif op == \"delete\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos+1:]\n",
    "        elif op == \"substitute\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            char = chr(random.randint(97, 122))\n",
    "            typo = word[:pos] + char + word[pos+1:]\n",
    "        elif op == \"transpose\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 2)\n",
    "            typo = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        elif op == \"repeat\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos] + word[pos:]  # lặp lại ký tự\n",
    "        elif op == \"wrong_tone\" and any(c in vowels for c in word):\n",
    "            pos = random.choice([i for i, c in enumerate(word) if c in vowels])\n",
    "            typo = word[:pos] + random_tone(word[pos]) + word[pos+1:]\n",
    "        # Có thể mở rộng thêm các quy tắc phát âm khác\n",
    "        if typo and typo != word and typo not in typo_list:\n",
    "            typo_list.append(typo)\n",
    "    return typo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3cac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc danh sách từ\n",
    "with open(\"vietnamese_only_tokens_sorted.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Sinh lỗi cho từng từ và lưu ra file\n",
    "with open(\"typos_for_all_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in words:\n",
    "        typos = generate_typos(word, num_typos=50)  # Số lỗi mỗi từ, có thể thay đổi\n",
    "        for typo in typos:\n",
    "            f.write(f\"{word}\\t{typo}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (venv39)",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
