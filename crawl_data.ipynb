{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e72341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã thu thập 50 link bài viết.\n",
      "Đã lưu 50 bài viết vào file raw_articles.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_vnexpress_articles(num_articles=50):\n",
    "    url = \"https://vnexpress.net\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    articles = []\n",
    "    links = []\n",
    "\n",
    "    # Lấy các thẻ chứa đường link bài viết chính\n",
    "    for a in soup.select(\"a[href^='https://vnexpress.net']\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and href not in links and len(links) < num_articles:\n",
    "            links.append(href)\n",
    "\n",
    "    print(f\"Đã thu thập {len(links)} link bài viết.\")\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            r = requests.get(link, timeout=5)\n",
    "            s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Lấy nội dung chính (thẻ article)\n",
    "            body = s.find(\"article\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "                if len(content) > 100:\n",
    "                    articles.append(content)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Gộp toàn bộ nội dung và lưu\n",
    "    full_text = \"\\n\".join(articles)\n",
    "    with open(\"raw_articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "\n",
    "    print(f\"Đã lưu {len(articles)} bài viết vào file raw_articles.txt\")\n",
    "\n",
    "# Gọi hàm crawl\n",
    "crawl_vnexpress_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74432d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc và sắp xếp 1745 từ tiếng Việt duy nhất.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from underthesea import word_tokenize\n",
    "import os\n",
    "\n",
    "# 1. Đưa về chữ thường, loại bỏ dấu câu, số, ký tự đặc biệt, loại bỏ khoảng trắng dư thừa\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡ'\n",
    "                  r'ùúụủũưừứựửữỳýỵỷỹđ\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 2. Đọc dữ liệu thô và làm sạch\n",
    "with open('raw_articles.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "# 3. Tách từ bằng underthesea\n",
    "tokenized_text = word_tokenize(cleaned_text, format=\"text\")\n",
    "tokens = tokenized_text.replace(\"_\", \" \").split()\n",
    "\n",
    "# 4. Lọc từ tiếng Việt\n",
    "def load_all_vietnamese_words(folder_path):\n",
    "    all_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    word = line.strip().lower()\n",
    "                    if word:\n",
    "                        all_words.add(word)\n",
    "    return all_words\n",
    "\n",
    "folder_path = 'tu_dien_goc'  # Thư mục chứa các file từ điển\n",
    "vietnamese_dict = load_all_vietnamese_words(folder_path)\n",
    "vietnamese_only_tokens = [word for word in tokens if word in vietnamese_dict]\n",
    "\n",
    "# 5. Loại bỏ từ trùng lặp và sắp xếp theo thứ tự abc\n",
    "unique_sorted = sorted(set(vietnamese_only_tokens), key=lambda x: x.lower())\n",
    "\n",
    "# 6. Lưu kết quả ra file\n",
    "with open('vietnamese_only_tokens_sorted.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in unique_sorted:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print(f\"Đã lọc và sắp xếp {len(unique_sorted)} từ tiếng Việt duy nhất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a376d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import unicodedata\n",
    "\n",
    "vowels = \"aăâeêioôơuưy\"\n",
    "tone_marks = [\"́\", \"̀\", \"̉\", \"̃\", \"̣\"]\n",
    "\n",
    "def remove_tone(char):\n",
    "    decomposed = unicodedata.normalize('NFD', char)\n",
    "    base = ''.join([c for c in decomposed if c not in tone_marks])\n",
    "    return unicodedata.normalize('NFC', base)\n",
    "\n",
    "def random_tone(char):\n",
    "    if char not in vowels:\n",
    "        return char\n",
    "    base = unicodedata.normalize('NFD', char)[0]\n",
    "    tone = random.choice(tone_marks)\n",
    "    return unicodedata.normalize('NFC', base + tone)\n",
    "\n",
    "def confused_initials(word):\n",
    "    # Các nhóm phụ âm đầu dễ nhầm lẫn\n",
    "    confusion_groups = [\n",
    "        [\"ch\", \"tr\"],\n",
    "        [\"l\", \"n\"],\n",
    "        [\"d\", \"r\", \"gi\"],\n",
    "        [\"s\", \"x\"],\n",
    "        [\"c\", \"k\"],\n",
    "        ['ngh', 'ng',\"gh\"],\n",
    "    ]\n",
    "    results = []\n",
    "    for group in confusion_groups:\n",
    "        for prefix in group:\n",
    "            if word.startswith(prefix):\n",
    "                for alt in group:\n",
    "                    if alt != prefix:\n",
    "                        results.append(alt + word[len(prefix):])\n",
    "                return results  # chỉ đổi 1 nhóm đầu tiên tìm thấy\n",
    "    return []\n",
    "\n",
    "def generate_typos(word, num_typos=10):\n",
    "    typo_list = []\n",
    "    operations = [\n",
    "        \"repeat\",      # Thừa chữ\n",
    "        \"delete\",      # Thiếu chữ\n",
    "        \"remove_tone\", # Thiếu dấu\n",
    "        \"wrong_tone\",  # Sai dấu\n",
    "        \"transpose\",   # Sai thứ tự chữ\n",
    "        \"confused_initial\", # Nhầm phụ âm đầu\n",
    "    ]\n",
    "    for _ in range(num_typos):\n",
    "        op = random.choice(operations)\n",
    "        typo = None\n",
    "        if op == \"repeat\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos] + word[pos:]\n",
    "        elif op == \"delete\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            typo = word[:pos] + word[pos+1:]\n",
    "        elif op == \"remove_tone\" and any(c in vowels for c in word):\n",
    "            pos = random.choice([i for i, c in enumerate(word) if c in vowels])\n",
    "            typo = word[:pos] + remove_tone(word[pos]) + word[pos+1:]\n",
    "        elif op == \"wrong_tone\" and any(c in vowels for c in word):\n",
    "            pos = random.choice([i for i, c in enumerate(word) if c in vowels])\n",
    "            typo = word[:pos] + random_tone(word[pos]) + word[pos+1:]\n",
    "        elif op == \"transpose\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word) - 2)\n",
    "            typo = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        elif op == \"confused_initial\":\n",
    "            confused = confused_initials(word)\n",
    "            if confused:\n",
    "                typo = random.choice(confused)\n",
    "        if typo and typo != word and typo not in typo_list:\n",
    "            typo_list.append(typo)\n",
    "    return typo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3cac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc danh sách từ\n",
    "with open(\"vietnamese_only_tokens_sorted.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Sinh lỗi cho từng từ và lưu ra file\n",
    "with open(\"typos_for_all_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in words:\n",
    "        typos = generate_typos(word, num_typos=50)  # Số lỗi mỗi từ, có thể thay đổi\n",
    "        for typo in typos:\n",
    "            f.write(f\"{word}\\t{typo}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70283c09-cc4a-474d-9857-bf63adb8aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số câu: 1588\n",
      "Ví dụ: ['\\nHà NộiBùi Lam Anh, 23 tuổi, bị tạm giữ hình sự để làm rõ cáo buộc chửi bới, quật ngã cảnh sát giao thông khi bị dừng xe kiểm tra.', 'TP HCM38 năm trước, bác sĩ Trần Thành Trai thực hiện ca mổ tách cặp dính liền Song - Pha, sau đó hai bé được một tỷ phú nhận con nuôi đưa sang Pháp và chưa một lần về lại quê hương.', 'Bộ tư lệnh không quân Ukraine cho biết Nga hôm nay tiến hành cuộc tập kích hiệp đồng nhằm vào nước này bằng 3 tên lửa đạn đạo Iskander-M, 4 tên lửa hành trình Kh-101 và Iskander-K, cùng 472 máy bay không người lái (UAV) tự sát Geran-2 và phi cơ mồi nhử.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    # Câu kết thúc bằng . ? !\n",
    "    return re.split(r'(?<=[.?!])\\s+', text)\n",
    "\n",
    "with open(\"raw_articles.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "sentences = split_sentences(full_text)\n",
    "print(\"Số câu:\", len(sentences))\n",
    "print(\"Ví dụ:\", sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "552c2923-8bf8-4b90-90a7-1c7aaabb6eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gốc: Một năm trước, Nga triển khai 30 UAV tấn công Ukraine trong một đêm là điều hiếm thấy.\n",
      "Lỗi: Một năm trước, Nga triển khai 30 UAV tấn công Ukraine trong một đm là điều hiếm thấy.\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "#from generate_typos import generate_typos  # dùng hàm của bạn\n",
    "\n",
    "def introduce_typos_to_sentence(sentence, max_typos=1):\n",
    "    words = sentence.split()\n",
    "    indices = list(range(len(words)))\n",
    "    typo_words = words.copy()\n",
    "    typo_count = 0\n",
    "\n",
    "    for _ in range(max_typos):\n",
    "        idx = choice(indices)\n",
    "        typos = generate_typos(words[idx], num_typos=3)\n",
    "        if typos:\n",
    "            typo_words[idx] = choice(typos)\n",
    "            typo_count += 1\n",
    "\n",
    "    if typo_count == 0:\n",
    "        return None  # không sinh lỗi được\n",
    "    return ' '.join(typo_words)\n",
    "\n",
    "# Test\n",
    "example = sentences[11]\n",
    "typo_sent = introduce_typos_to_sentence(example)\n",
    "print(\"Gốc:\", example)\n",
    "print(\"Lỗi:\", typo_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d4e996-9f09-4722-92d5-e20549b82593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ sai: [(1, 'đag'), (3, 'com'), (7, 'hang'), (8, '.')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# Tải từ điển tiếng Việt (chứa từ thường)\n",
    "with open(\"vietnamese_only_tokens_sorted.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vietnamese_dict = set([line.strip() for line in f if line.strip()])\n",
    "\n",
    "# Hàm loại bỏ dấu câu và chuẩn hóa từ để kiểm tra\n",
    "def is_valid_word(word):\n",
    "    # Bỏ ký tự không phải chữ cái (ví dụ: dấu chấm, phẩy)\n",
    "    word = re.sub(r'[^\\wàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễ'\n",
    "                  r'ìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữ'\n",
    "                  r'ỳýỵỷỹđ]', '', word.lower())\n",
    "    return word in vietnamese_dict\n",
    "\n",
    "# Hàm phát hiện từ sai\n",
    "def detect_misspelled_words(text, dictionary):\n",
    "    tokens = word_tokenize(text, format=\"text\").split()\n",
    "    misspelled = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if not is_valid_word(word):\n",
    "            misspelled.append((i, word))\n",
    "    return tokens, misspelled\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "input_text = \"Tôi đag ăn com trưa ở nhà hang.\"\n",
    "tokens, misspelled = detect_misspelled_words(input_text, vietnamese_dict)\n",
    "\n",
    "print(\"Các từ sai:\", misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157c2477-cd88-46c1-809e-51ab80cd84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# Đọc dữ liệu\n",
    "with open(\"raw_articles.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Tách câu → tách từ → thành tập train\n",
    "sentences = raw_text.split('\\n')  # hoặc dùng split_sentences()\n",
    "tokenized_sentences = [word_tokenize(sentence, format=\"text\").split() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Huấn luyện Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Lưu lại model\n",
    "w2v_model.save(\"word2vec_vietnamese.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2298e21a-6c92-4791-9f13-56fcdf2ee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "# Hàm gợi ý sửa từ sai\n",
    "def suggest_correction(word, model, dictionary, topn=5):\n",
    "    # 1. Tìm từ gần đúng trong từ điển dựa trên khoảng cách ký tự\n",
    "    candidates = get_close_matches(word, dictionary, n=20, cutoff=0.7)\n",
    "    \n",
    "    # 2. Tìm trong số đó những từ có vector trong mô hình\n",
    "    valid_candidates = [w for w in candidates if w in model.wv]\n",
    "\n",
    "    # 3. Chọn top-n từ gần nhất trong không gian ngữ nghĩa\n",
    "    suggestions = []\n",
    "    for w in valid_candidates:\n",
    "        sim = model.wv.similarity(word, w) if word in model.wv else 0\n",
    "        suggestions.append((w, sim))\n",
    "    \n",
    "    suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [w for w, _ in suggestions[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0b5834-db27-48aa-b332-4db6b9e2634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gợi ý sửa: ['đang']\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec_vietnamese.model\")\n",
    "suggestion = suggest_correction(\"đag\", w2v_model, vietnamese_dict)\n",
    "print(\"Gợi ý sửa:\", suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c324d7ac-a7aa-4fed-bbd4-7ceaa927aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ví dụ:\n",
      "Sai: Hà_NộiBùi Lam_Anh , 23 tuổi , bị tạm giữ hình_sự để làm rõ cáo_buộc chửi_bới , uqật ngã cảnh_sát giao_thông khi bị dnừg xe kiểm_tra .\n",
      "Đúng: Hà NộiBùi Lam Anh, 23 tuổi, bị tạm giữ hình sự để làm rõ cáo buộc chửi bới, quật ngã cảnh sát giao thông khi bị dừng xe kiểm tra.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "# Danh sách câu đúng từ tập crawl\n",
    "with open(\"raw_articles.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Hàm tạo lỗi đơn giản bằng hoán đổi ký tự\n",
    "def introduce_typos(sentence, typo_prob=0.2):\n",
    "    def corrupt(word):\n",
    "        if random.random() < typo_prob and len(word) > 3:\n",
    "            pos = random.randint(0, len(word) - 2)\n",
    "            return word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        return word\n",
    "\n",
    "    words = word_tokenize(sentence, format=\"text\").split()\n",
    "    corrupted = [corrupt(w) for w in words]\n",
    "    return ' '.join(corrupted)\n",
    "\n",
    "# Tạo dữ liệu lỗi\n",
    "pairs = []\n",
    "for s in raw_sentences:\n",
    "    corrupted = introduce_typos(s)\n",
    "    if corrupted != s:\n",
    "        pairs.append((corrupted, s))\n",
    "\n",
    "print(\"Ví dụ:\")\n",
    "print(\"Sai:\", pairs[0][0])\n",
    "print(\"Đúng:\", pairs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5bbd5e1-795a-4efc-ac64-b20303f97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"Toi an com\", \"tôi ăn cơm\"),\n",
    "    (\"Toi hoc bai\", \"tôi học bài\"),\n",
    "    (\"Toi di ngu\", \"tôi đi ngủ\"),\n",
    "    (\"Toi uong nuoc\", \"tôi uống nước\"),\n",
    "    (\"Toi di hoc\", \"tôi đi học\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2ea6e95-d4e1-435e-ab70-89d2ef615e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "input_texts = [x[0] for x in pairs]\n",
    "target_texts = ['<start> ' + x[1] + ' <end>' for x in pairs]\n",
    "\n",
    "tokenizer = Tokenizer(filters='', lower=True, oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "input_tensor = pad_sequences(tokenizer.texts_to_sequences(input_texts), padding='post')\n",
    "target_tensor = pad_sequences(tokenizer.texts_to_sequences(target_texts), padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_input_len = input_tensor.shape[1]\n",
    "max_target_len = target_tensor.shape[1]\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "807d8bcd-17a2-4ae5-be76-1e8d850cda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                </span>┃<span style=\"font-weight: bold\"> Output Shape            </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to            </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                       │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                       │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │ input_layer_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │                │                         │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],           │\n",
       "│                             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                   │                │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]            │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">5,654</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└─────────────────────────────┴─────────────────────────┴────────────────┴─────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to           \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_13 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                       │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                       │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m2,816\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m2,816\u001b[0m │ input_layer_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │        \u001b[38;5;34m394,240\u001b[0m │ embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                             │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │                │                         │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │        \u001b[38;5;34m394,240\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],           │\n",
       "│                             │ \u001b[38;5;34m256\u001b[0m)]                   │                │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]            │\n",
       "├─────────────────────────────┼─────────────────────────┼────────────────┼─────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)        │          \u001b[38;5;34m5,654\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└─────────────────────────────┴─────────────────────────┴────────────────┴─────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">799,766</span> (3.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m799,766\u001b[0m (3.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">799,766</span> (3.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m799,766\u001b[0m (3.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "embedding_dim = 128\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90dbae63-b5b7-4cd3-82aa-3686085b175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.2031 - loss: 3.0890\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5000 - loss: 3.0369\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5000 - loss: 2.9622\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5000 - loss: 2.8252\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5000 - loss: 2.5677\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 2.1700\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 2.2016\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5000 - loss: 1.9098 \n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5000 - loss: 1.8182\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5000 - loss: 1.6032\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5000 - loss: 1.5186\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5250 - loss: 1.4810\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6125 - loss: 1.2034\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6219 - loss: 1.0974\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6781 - loss: 1.0767\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5969 - loss: 0.9306\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7344 - loss: 0.8728\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6531 - loss: 0.8114\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6781 - loss: 0.7526\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6625 - loss: 0.7166\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.6719 - loss: 0.7012\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7656 - loss: 0.6089\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8219 - loss: 0.5326\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8375 - loss: 0.4631\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9344 - loss: 0.4212\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9187 - loss: 0.3496\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9594 - loss: 0.3229\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9031 - loss: 0.3078\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.2563\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9750 - loss: 0.1951\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1528\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1674\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1195\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0864\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.1071\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0814\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0555\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0577\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0481\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0386\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0356\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0305\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0236\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0225\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0212\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0197\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0185 \n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0204\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0179\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0148\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0159\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0138\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0140\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0126\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0121\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0115\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0113\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0111\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0102\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0097\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0092\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0096\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0086\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0084\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0086\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0078\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0076\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0079\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0072\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0075\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0068\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0064\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0070\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0067\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0059\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0058\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0054\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0060\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0052\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0056 \n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0055\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0051\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0040\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0039\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d2ffb403d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor_in = target_tensor[:, :-1]\n",
    "target_tensor_out = target_tensor[:, 1:]\n",
    "\n",
    "target_tensor_out = np.expand_dims(target_tensor_out, -1)\n",
    "\n",
    "model.fit([input_tensor, target_tensor_in], target_tensor_out, batch_size=2, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbefdd8f-36fd-4637-8a54-021633b4c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model để dự đoán trạng thái từ câu đầu vào\n",
    "encoder_model = Model(encoder_inputs, [state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb39ca03-bf75-49ea-bf2c-1fc387cc16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Các input cho decoder khi suy diễn\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Input chính cho decoder\n",
    "decoder_inputs = Input(shape=(1,))  # chỉ 1 bước thời gian tại mỗi vòng lặp\n",
    "\n",
    "# Lớp embedding (phải giống với training)\n",
    "decoder_embedding_layer = Embedding(vocab_size, embedding_dim)\n",
    "dec_emb = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "# LSTM trong chế độ suy diễn\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb, initial_state=decoder_states_inputs)\n",
    "\n",
    "# Lớp Dense để dự đoán từ tiếp theo\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Trả về cả output và state để dùng trong bước tiếp theo\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# Xây dựng mô hình decoder\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6451c8f-7fc5-4757-b654-a62dbb92f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
    "\n",
    "    # Bước 1: mã hóa input để lấy trạng thái ban đầu\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Bắt đầu với token <start>\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == '<end>' or len(decoded_sentence) > max_target_len:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "\n",
    "            # Cập nhật target_seq và trạng thái tiếp theo\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "            states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11395551-f7f3-482d-aa62-1ac8e5b9f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Sửa: tôi ăn cơm\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Sửa: tôi uống nước\n"
     ]
    }
   ],
   "source": [
    "print(\"Sửa:\", decode_sequence(\"Toi an com\"))\n",
    "print(\"Sửa:\", decode_sequence(\"Toi ún nuoc\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
